# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from transformers import PretrainedConfig


class MiniMindConfig(PretrainedConfig):
    model_type = "minimind"

    def __init__(
            self,
            dropout: float = 0.0,
            bos_token_id: int = 1,
            eos_token_id: int = 2,
            hidden_act: str = 'silu',
            hidden_size: int = 512,
            intermediate_size: int = None,
            max_position_embeddings: int = 32768,
            num_attention_heads: int = 8,
            num_hidden_layers: int = 8,
            num_key_value_heads: int = 2,
            vocab_size: int = 6400,
            rms_norm_eps: float = 1e-05,
            rope_theta: int = 1000000.0,
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            use_moe: bool = False,
            num_experts_per_tok: int = 2,
            n_routed_experts: int = 4,
            n_shared_experts: int = 1,
            scoring_func: str = 'softmax',
            aux_loss_alpha: float = 0.1,
            seq_aux: bool = True,
            norm_topk_prob: bool = True,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

import math
import torch
from torch import nn
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
import torch.nn.functional as F
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast


class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps # ä¸€ä¸ªå°çš„å¸¸æ•°ï¼Œç”¨äºé˜²æ­¢é™¤ä»¥é›¶
        self.gamma = nn.Parameter(torch.ones(dim)) # weight.shape = (dim,)ï¼Œå³æ¯ä¸ªç‰¹å¾ä¸€ä¸ªç¼©æ”¾å‚æ•°ï¼Œåˆå§‹å€¼ä¸º 1;æœ€ç»ˆçš„ä½œç”¨ï¼šåœ¨å½’ä¸€åŒ–ä¹‹åï¼Œå†ä¸ºæ¯ä¸ªç»´åº¦åŠ ä¸€ä¸ª å¯å­¦ä¹ çš„ç¼©æ”¾ï¼›nn.Parameterï¼šè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ª å¯å­¦ä¹ å‚æ•°ï¼Œè®­ç»ƒæ—¶ä¼šæ›´æ–°

    def _norm(self, x):
        # è®¡ç®—è¾“å…¥xåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šçš„å¹³æ–¹å‡å€¼ï¼ŒåŠ ä¸Šepsåå¼€å¹³æ–¹å–å€’æ•°ï¼ˆrsqrtï¼‰,ç”¨è¾“å…¥xä¹˜ä»¥è¿™ä¸ªå€’æ•°ï¼Œå®ç°å½’ä¸€åŒ–
        return x * torch.rsqrt(  # ç›´æ¥è°ƒç”¨ rsqrt æ¯”å…ˆ sqrt å† 1 / æ›´é«˜æ•ˆï¼Œå°¤å…¶åœ¨ GPU ä¸Š
            x.pow(2).mean(-1, keepdim=True) 
            + self.eps) 

    def forward(self, x):
        return self.gamma * self._norm(x.float()).type_as(x)

# è®¡ç®—RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰çš„cosã€sin é¢‘ç‡
def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    """
    é¢„è®¡ç®—RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰çš„æ—‹è½¬é¢‘ç‡
    Args:
        dim (int): åµŒå…¥ç»´åº¦ï¼Œå³æ¯ä¸ªtokençš„ç‰¹å¾ç»´åº¦
        end (int, optional): åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œå³ä½ æƒ³è¦é¢„è®¡ç®—å¤šå°‘ä¸ªä½ç½®çš„ cos/sin, é»˜è®¤ä¸º32kï¼Œç›¸å½“äºæå‰ç”Ÿæˆ 32k ä¸ªä½ç½®çš„ cos/sin
        theta (float, optional): é¢‘ç‡è®¡ç®—çš„åŸºæ•°å‚æ•°ï¼Œé»˜è®¤ä¸º1e6,LLaMA é‡Œå°±æ˜¯è¿™ä¹ˆè®¾çš„
    """
    # ç”Ÿæˆå¶æ•°ç´¢å¼• [0, 2, 4, ...]ï¼Œå› ä¸º RoPE é€šå¸¸å¯¹æ¯ä¸¤ä¸ªç»´åº¦ä¸€ç»„è¿›è¡Œæ—‹è½¬
    freqs = 1.0 / (
        theta ** (
            torch.arange(0, dim, 2
                         )[: (dim // 2)].float(
                             ) / dim))
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float() # shape = (end, dim//2)
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """
    ä¸º Q/K å‘é‡åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRotary Positional Embedding, RoPEï¼‰ã€‚

    âš¡ ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨äº†æ—‹è½¬å‰ç½®ï¼ˆpre-rotationï¼‰æ–¹æ³•ï¼šé€šè¿‡ `rotate_half` å°†å‘é‡çš„å‰ååŠæ®µé‡æ–°æ’åˆ—å¹¶å–è´Ÿï¼Œ
      ç„¶åä¸ cos/sin è¿›è¡Œçº¿æ€§ç»„åˆã€‚
    - è™½ç„¶å†™æ³•ä¸æ ‡å‡† RoPE å…¬å¼ç•¥æœ‰ä¸åŒï¼Œä½†æ•°å­¦ç»“æœä¸å…¬å¼å®Œå…¨ç­‰ä»·ã€‚
    
    å‚æ•°ï¼š
    - q: torch.Tensor, shape = (batch, seq_len, dim)ï¼ŒQuery å‘é‡
    - k: torch.Tensor, shape = (batch, seq_len, dim)ï¼ŒKey å‘é‡
    - cos: torch.Tensor, shape = (seq_len, dim)ï¼Œé¢„è®¡ç®—çš„ cos çŸ©é˜µ
    - sin: torch.Tensor, shape = (seq_len, dim)ï¼Œé¢„è®¡ç®—çš„ sin çŸ©é˜µ
    - position_ids: å¯é€‰ä½ç½®ç´¢å¼•ï¼ˆæœªä½¿ç”¨ï¼‰
    - unsqueeze_dim: æ‰©å±• cos/sin çš„ç»´åº¦ï¼Œç”¨äºå¹¿æ’­ä¹˜æ³•
    
    è¿”å›ï¼š
    - q_embed, k_embed: åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åçš„ Q/K å‘é‡ï¼Œshape ä¸è¾“å…¥ç›¸åŒ

    è¯´æ˜ï¼š
    RoPE çš„æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹æ¯å¯¹ embedding ç»´åº¦è¿›è¡ŒäºŒç»´æ—‹è½¬ï¼Œå°†ä½ç½®ä¿¡æ¯ç¼–ç è¿› Q/Kã€‚
    æ­¤å®ç°é€šè¿‡å‰ç½®æ—‹è½¬å®ç°åŒæ ·æ•ˆæœï¼Œä¾¿äºè®¡ç®—å’Œå¹¿æ’­ã€‚
    """    
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed

# å°†é”®å€¼å¯¹å¼ é‡æ²¿ç€æ³¨æ„åŠ›å¤´çš„ç»´åº¦è¿›è¡Œå¤åˆ¶æ‰©å±•ã€‚
# åœ¨MQAã€GQAä¸­ï¼Œé”®(key)å’Œå€¼(value)å¼ é‡çš„å¤´æ•°å°‘äºæŸ¥è¯¢(query)å¼ é‡çš„å¤´æ•°ï¼Œ
# éœ€è¦é€šè¿‡å¤åˆ¶æ‰©å±•é”®å’Œå€¼å¼ é‡æ¥åŒ¹é…æŸ¥è¯¢å¼ é‡çš„å¤´æ•°ã€‚
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1: # å¦‚æœ n_rep == 1ï¼Œåˆ™ç›´æ¥è¿”å›è¾“å…¥å¼ é‡ x
        return x
    # å¦åˆ™ï¼Œåœ¨ç¬¬4ä¸ªç»´åº¦æ’å…¥ä¸€ä¸ªæ–°ç»´åº¦å¹¶æ‰©å±•ï¼Œç„¶åé‡æ–°reshapeï¼Œå®ç°é”®å€¼å¤´çš„é‡å¤å¤åˆ¶
    return (
        x[:, :, :, None, :]
        .expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    # Attention åˆå§‹åŒ–
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        # è®¾ç½®KVå¤´æ•°é‡(GQA,å¤šä¸ª Query å¤´å…±äº«åŒä¸€ä¸ª KV å¤´)ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä¸æ³¨æ„åŠ›å¤´æ•°é‡ç›¸åŒ(MHA,æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›)
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0 # ç¡®ä¿æ³¨æ„åŠ›å¤´æ•°é‡èƒ½è¢«KVå¤´æ•°é‡æ•´é™¤
        # è®¾ç½®æ³¨æ„åŠ›å¤´å’ŒKVå¤´æ•°é‡
        self.n_local_heads = args.num_attention_heads
        self.n_local_kv_heads = self.num_key_value_heads
        # æ¯ä¸ª KV head è¢«å¤šå°‘ä¸ª Q head å…±äº«
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
        self.head_dim = args.hidden_size // args.num_attention_heads # hidden_sizeè¡¨ç¤ºæ¯ä¸ªtokençš„ç»´åº¦ï¼Œä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´æ‹†åˆ†æˆå¤šä¸ªå°å‘é‡
        
        # åˆå§‹åŒ–QKVçº¿æ€§æ˜ å°„å±‚
        # å°†è¾“å…¥ token embedding æ˜ å°„ä¸º Q/K/V å‘é‡
        # æ³¨æ„: é€šè¿‡çº¿æ€§å±‚å°†æ•´ä¸ª hidden_size æ˜ å°„åˆ°æ¯ä¸ª head çš„ head_dimï¼Œè€Œä¸æ˜¯ç›´æ¥æŠŠ hidden_size åˆ‡æˆ num_heads ä»½ã€‚
        #       å¦‚æœç›´æ¥åˆ‡åˆ†ï¼Œæ¯ä¸ªå¤´åªèƒ½çœ‹åˆ°éƒ¨åˆ† hidden_sizeï¼Œè¡¨è¾¾èƒ½åŠ›ä¼šå—é™ã€‚
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)  # Q æ€»æ˜¯æŠ•å½±æˆ å®Œæ•´çš„ num_heads
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)  # K/V æŠ•å½±çš„ head æ•°å–å†³äº num_key_value_heads
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)  # K/V æŠ•å½±çš„ head æ•°å–å†³äº num_key_value_heads
        
        # åˆå§‹åŒ–è¾“å‡ºæŠ•å½±å±‚ï¼Œå°†å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºæ‹¼æ¥å› hidden_size
        # æ³¨æ„: è¿™é‡Œä¸æ˜¯ç®€å•æ‹¼æ¥ï¼Œè€Œæ˜¯é€šè¿‡çº¿æ€§å±‚è¿›è¡Œå¯å­¦ä¹ çš„æ··åˆï¼Œä½¿æ¯ä¸ª hidden_size ç»´åº¦èƒ½æ•´åˆæ¥è‡ªä¸åŒå¤´çš„ä¿¡æ¯ã€‚
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        
        # åˆå§‹åŒ–æ³¨æ„åŠ›dropoutå’Œæ®‹å·®dropout
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self,
                x: torch.Tensor, # è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, hidden_size)
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # æ—‹è½¬ä½ç½®ç¼–ç å…ƒç»„ï¼ŒåŒ…å«(cos, sin)ä¸¤ä¸ªå¼ é‡
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, # ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„keyå’Œvalueç¼“å­˜ï¼Œç”¨äºåŠ é€Ÿè§£ç 
                use_cache=False, # æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œç”¨äºç”Ÿæˆæ—¶çš„kv_cache
                
                # æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºå±è”½paddingä½ç½®ï¼ˆpaddingçš„tokenä¸å‚ä¸æ³¨æ„åŠ›è®¡ç®—ï¼‰ã€‚è¾“å…¥åºåˆ—: [æˆ‘, çˆ±, è‡ªç„¶, è¯­è¨€, <pad>, <pad>]attention_mask: [1, 1, 1, 1, 0, 0],å€¼ä¸º 1 è¡¨ç¤ºè¿™ä¸ªä½ç½®æ˜¯æœ‰æ•ˆ tokenï¼Œå€¼ä¸º 0 è¡¨ç¤ºè¿™ä¸ªä½ç½®æ˜¯ paddingï¼Œä¸åº”è¯¥å‚ä¸æ³¨æ„åŠ›
                # ä¸€ä¸ª batch å«å¤šä¸ªé•¿çŸ­ä¸ä¸€çš„åºåˆ—ï¼Œæ‰€ä»¥è¦åŠ paddingå¡«å……åˆ°åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆseq_lenï¼‰
                attention_mask: Optional[torch.Tensor] = None): 
        bsz, seq_len, _ = x.shape
        # æŠ•å½± çº¿æ€§å˜æ¢ -> Q, K, V: çº¿æ€§å±‚ç”Ÿæˆæ‰€æœ‰å¤´,ä¸€æ¬¡æ€§æŠŠæ¯ä¸ª token æ˜ å°„åˆ° æ‰€æœ‰ Q/K/V å¤´æ‹¼åœ¨ä¸€èµ·çš„ç©ºé—´
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        # å†reshapeæ‹†æˆå¤šä¸ªå¤´
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        # æ—‹è½¬ä½ç½®ç¼–ç 
        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # kv_cacheå®ç°, å°†å†å²çš„ key/value ä¸å½“å‰æ‹¼æ¥(è®­ç»ƒæ—¶ä¸éœ€è¦)
        if past_key_value is not None:
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        present_kv = (xk, xv) if use_cache else None

        # é‡å¤ KV å¤´ & è½¬ç½®: è®©æ‰€æœ‰ Q head å¯¹åº”åˆ°æ­£ç¡®çš„ KV head
        # åœ¨ GQAæˆ– MQA ä¸­ï¼Œå¤šä¸ª Q å¤´å…±äº«åŒä¸€ä¸ª KV å¤´,å¦‚æœç›´æ¥åš QÂ·K^Tï¼Œä¼šæŠ¥é”™ï¼Œå› ä¸ºå¤´æ•°ä¸åŒ¹é…,è§£å†³æ–¹æ¡ˆï¼šrepeat_kvä½œç”¨ï¼šæŠŠå°‘é‡ KV å¤´ é‡å¤ n_rep æ¬¡,è®©æ¯ä¸ª Q å¤´éƒ½å¯¹åº”ä¸€ä¸ª KV å¤´
        xq, xk, xv = (
            xq.transpose(1, 2), # [batch, seq_len, num_heads, head_dim] â†’ [batch, num_heads, seq_len, head_dim]
            repeat_kv(xk, self.n_rep  # xk [batch, seq_len, num_key_value_heads, head_dim] â†’ [batch, seq_len, num_key_value_heads*n_rep, head_dim] = [batch, seq_len, num_heads, head_dim] 
                      ).transpose(1, 2), # â†’ [batch, num_heads, seq_len, head_dim]
            repeat_kv(xv, self.n_rep  # xv size å˜åŒ–åŒä¸Š
                      ).transpose(1, 2) 
        )

        # è®¡ç®—æ³¨æ„åŠ›
        if self.flash and seq_len != 1: # ä½¿ç”¨ FlashAttention
            dropout_p = self.dropout if self.training else 0.0
            attn_mask = None
            if attention_mask is not None:
                attn_mask = attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1)
                attn_mask = attn_mask.bool() if attention_mask is not None else None
            # ä½¿ç”¨ PyTorch åŸç”Ÿ flash attentionï¼ˆå†…ç½® causalï¼‰
            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)
            
        # ä½¿ç”¨ä¼ ç»Ÿçš„æ³¨æ„åŠ›è®¡ç®—
        else: 
            # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [batch, num_heads, seq_len, seq_len]
            
            # æ·»åŠ ä¸Šä¸‰è§’ maskï¼Œå®ç° causal attentionã€‚scores+maskï¼Œ ä¸Šä¸‰è§’å˜æˆ-infï¼Œç»è¿‡softmaxåè¶‹äº0ï¼Œä»è€Œé®ç›–æœªæ¥ä¿¡æ¯
            casual_mask=torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),  # åˆ›å»ºä¸€ä¸ªå¤§å°ä¸º (seq_len, seq_len) çš„çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ -infã€‚
                diagonal=1 # æ§åˆ¶ä»å“ªæ¡å¯¹è§’çº¿å¼€å§‹ä¿ç•™ä¸Šä¸‰è§’ã€‚åœ¨ causal mask ä¸­ä½¿ç”¨ diagonal=1ï¼Œè¡¨ç¤ºå±è”½ä¸»å¯¹è§’çº¿å³ä¸Šæ–¹çš„æœªæ¥ä½ç½®ï¼Œä¿ç•™å½“å‰ token è‡ªå·±å’Œä¹‹å‰çš„ tokenã€‚
            ).unsqueeze(0).unsqueeze(0)  # unsqueezeï¼šåŒ¹é… batch å’Œ head ç»´åº¦
            scores = scores + casual_mask
            
            # attention æ©ç ï¼ˆå¦‚ padding maskï¼‰
            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [bsz,seq_len]-->[bsz,1,1,seq_len]ã€‚
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9 # attention_maskåŸæ¥æ˜¯ 1 çš„åœ°æ–¹ï¼ˆæœ‰æ•ˆ tokenï¼‰å˜æˆ 0ï¼›åŸæ¥æ˜¯ 0 çš„åœ°æ–¹ï¼ˆpaddingï¼‰å˜æˆ -1e9ï¼ˆéå¸¸å°ï¼‰
                # å°† scores ä¸ extended_attention_mask ç›¸åŠ ï¼š
                # - causal maskï¼ˆscoresï¼‰ å·²ç»æŠŠæœªæ¥ token çš„åˆ†æ•°è®¾ä¸º -inf
                # - extended_attention_mask æŠŠ padding token çš„åˆ†æ•°è®¾ä¸º -1e9
                # è¿™æ ·ç»è¿‡ softmax åï¼Œæœªæ¥ä¿¡æ¯å’Œ padding ä¿¡æ¯çš„æ³¨æ„åŠ›æƒé‡éƒ½ â‰ˆ 0ï¼Œä¸è¢«æ¨¡å‹å…³æ³¨
                scores = scores + extended_attention_mask 
                
            # softmax + dropout
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            # æ³¨æ„åŠ›åŠ æƒæ±‚å’Œ
            output = scores @ xv  # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, head_dim) -> (batch, num_heads, seq_len, head_dim)

        # è¾“å‡ºæŠ•å½±ä¸æ®‹å·® dropout # è¿˜åŸè¾“å‡ºå½¢çŠ¶ï¼Œå¹¶é€šè¿‡è¾“å‡ºçº¿æ€§å±‚
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)  # -> transposeï¼š(batch, seq_len, num_heads, head_dim) -> reshapeï¼š(batch, seq_len, hidden_size)
        output = self.resid_dropout(self.o_proj(output))
        return output, present_kv



class FeedForward(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œå±‚ï¼ˆFeed Forward Networkï¼‰
    
    è¯¥å±‚æ˜¯Transformeræ¶æ„ä¸­çš„å‰é¦ˆç½‘ç»œç»„ä»¶ï¼Œé‡‡ç”¨é—¨æ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰ç»“æ„ï¼Œ
    åŒ…å«ä¸ŠæŠ•å½±ã€é—¨æ§æŠ•å½±å’Œä¸‹æŠ•å½±ä¸‰ä¸ªçº¿æ€§å˜æ¢ï¼Œä»¥åŠæ¿€æ´»å‡½æ•°å’Œdropoutã€‚
    
    å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼šé—¨æ§æœºåˆ¶å¯ä»¥åŠ¨æ€é€‰æ‹©å“ªäº›ç‰¹å¾é‡è¦
    æå‡æ¨¡å‹æ€§èƒ½ï¼šæ¯”æ™®é€š FFN æ›´å®¹æ˜“æ•æ‰å¤æ‚æ¨¡å¼
    è®­ç»ƒæ›´ç¨³å®šï¼šGEGLU/GLU ç»“æ„åœ¨å¤§æ¨¡å‹ä¸­æ¯”æ™®é€š FFN è¡¨ç°æ›´å¥½
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        # å¦‚æœ intermediate_sizeï¼ˆFFN çš„ä¸­é—´ç»´åº¦ï¼‰ æ²¡æŒ‡å®šï¼Œå°±æŒ‰ hidden_size * 8/3 æ¥è®¡ç®—ï¼Œç„¶ååš 64 å¯¹é½ï¼ˆå¸¸ç”¨ GPU ä¼˜åŒ–æ‰‹æ®µï¼Œä¿è¯ç»´åº¦æ˜¯ 64 çš„å€æ•°ï¼‰
        # è®¡ç®—å…¬å¼ï¼šå…ˆæŒ‰hidden_sizeçš„8/3å€è®¡ç®—ï¼Œç„¶åå‘ä¸Šå–æ•´åˆ°64çš„å€æ•°ã€‚(â€œ8/3â€ å…¶å®æ˜¯ä¸€ä¸ª ç»éªŒå…¬å¼ï¼Œæ¥è‡ª Transformer ç³»åˆ—æ¨¡å‹çš„è®¾è®¡ç»éªŒï¼Œå¹¶ä¸æ˜¯ä¸¥æ ¼çš„æ•°å­¦å®šå¾‹ã€‚å®ƒçš„ä½œç”¨æ˜¯å†³å®š FFN çš„ä¸­é—´å±‚ç»´åº¦ï¼ˆintermediate_sizeï¼‰ç›¸å¯¹äºéšè—ç»´åº¦ hidden_size çš„æ”¾å¤§æ¯”ä¾‹ã€‚)
        # ä¸ºä»€ä¹ˆé€‰æ‹© 64? 1.å‘é‡åŒ–æ•ˆç‡é«˜ï¼šGPU çš„ çº¿ç¨‹å— (warp) ä¸€èˆ¬æ˜¯ 32 ä¸ªçº¿ç¨‹ï¼Œå¾ˆå¤šåº•å±‚åº“ï¼ˆå¦‚ cuBLASã€TensorRTï¼‰åœ¨åšçŸ©é˜µä¹˜æ³•æ—¶ï¼Œä¼šæŒ‰ 32 æˆ– 64 å¯¹é½å—æ“ä½œã€‚2.å‡å°‘ç©ºæ´å¡«å……ï¼šå¦‚æœçŸ©é˜µç»´åº¦ä¸æ˜¯ 64 çš„å€æ•°ï¼ŒGPU ä¼šåœ¨è®¡ç®—æ—¶å¡«å……é›¶ï¼Œé€ æˆé¢å¤–å¼€é”€ã€‚3.è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼šå¯¹é½åˆ° 64 çš„å€æ•°å¯ä»¥æœ€å¤§åŒ–åˆ©ç”¨ GPU çš„è®¡ç®—å•å…ƒï¼Œä½¿çŸ©é˜µä¹˜æ³•æ•ˆç‡æœ€é«˜ã€‚
        if config.intermediate_size is None:  # ä¸­é—´å±‚ç»´åº¦,å¦‚æœæ²¡æŒ‡å®šå°±æŒ‰ 8/3 å€ idden_size ç®—
            intermediate_size = int(config.hidden_size * 8 / 3)  # åŒæ—¶ç¡®ä¿ä¸­é—´å±‚ç»´åº¦æ˜¯64çš„æ•´æ•°å€
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)  # é¦–å…ˆå¾—åˆ°ä¸­é—´å±‚
        self.act_fn = ACT2FN[config.hidden_act]                                               # æ¿€æ´»å‡½æ•°ï¼Œä¿¡å·å¼€å…³
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)     # ä¸»ä¿¡æ¯é€šé“ï¼Œå†æ¬¡æ˜ å°„åˆ°ä¸­é—´å±‚
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)   # å‹å›åŸç»´åº¦ï¼ŒæŠŠä¸­é—´ç»´åº¦å†å‹å› hidden_sizeï¼Œæ–¹ä¾¿å’Œåç»­æ¨¡å—ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›ï¼‰ç›¸åŠ 
        self.dropout = nn.Dropout(config.dropout)                                              # Dropoutå±‚ï¼Œéšæœºä¸¢å¼ƒéƒ¨åˆ†å…ƒç´ ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

    def forward(self, x):
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    """
    1.å¯¹æ¯ä¸ª token è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„æ¦‚ç‡ã€‚
    2.é€‰æ‹© top-k ä¸“å®¶å¹¶å¯å½’ä¸€åŒ–æƒé‡ã€‚
    3.æä¾›è¾…åŠ©æŸå¤±ï¼Œä¿è¯ä¸“å®¶è´Ÿè½½å‡è¡¡ã€‚
    4.è¾“å‡º top-k ä¸“å®¶ç´¢å¼•å’Œæƒé‡ï¼Œä¾› MoE çš„åç»­ä¸“å®¶å±‚ä½¿ç”¨ã€‚
    """
class MoEGate(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok # æ¯ä¸ª token å°†è¢«åˆ†é…åˆ° top-k ä¸ªä¸“å®¶
        self.n_routed_experts = config.n_routed_experts # æ€»å¯é€‰ä¸“å®¶æ•°ï¼ˆä¸å«å…±äº«ä¸“å®¶ï¼‰

        self.scoring_func = config.scoring_func # è¯„åˆ†å‡½æ•°ï¼ˆä»…æ”¯æŒ softmaxï¼‰
        self.alpha = config.aux_loss_alpha # è¾…åŠ©æŸå¤±æƒé‡ç³»æ•°ï¼ˆç”¨äºè®­ç»ƒæ—¶å¹³è¡¡ä¸“å®¶è´Ÿè½½ï¼‰
        self.seq_aux = config.seq_aux # æ˜¯å¦ä½¿ç”¨æŒ‰åºåˆ—ç»´åº¦çš„å‡è¡¡æŸå¤±ï¼ˆTrue/Falseï¼‰

        self.norm_topk_prob = config.norm_topk_prob # æ˜¯å¦å¯¹ top-k æƒé‡å½’ä¸€åŒ–
        self.gating_dim = config.hidden_size # é—¨æ§è¾“å…¥ token å‘é‡çš„ç»´åº¦ï¼ˆå³ hidden_sizeï¼‰
        
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim))) # é—¨æ§ç½‘ç»œçš„æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ [n_routed_experts, hidden_size]
        self.reset_parameters() # åˆå§‹åŒ–æƒé‡

    # åˆå§‹åŒ–MoEGate é—¨æ§ç½‘ç»œçš„æƒé‡
    def reset_parameters(self) -> None:
        import torch.nn.init as init
        init.kaiming_uniform_(self.weight, a=math.sqrt(5)) # ä½¿ç”¨ Kaiming å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡çŸ©é˜µï¼Œé€‚åˆ ReLU ç­‰éçº¿æ€§æ¿€æ´»

    def forward(self, hidden_states):  # MoEGate æ¥æ”¶ hidden_states ä¸ºè¾“å…¥ï¼ˆå³è¾“å…¥ token çš„è¡¨ç¤ºï¼‰
        bsz, seq_len, h = hidden_states.shape
        hidden_states = hidden_states.view(-1, h)  # å½¢çŠ¶é‡å¡‘ä¸º (batch_size * seq_len, hidden_size)ï¼Œä»¥ä¾¿ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ token
        logits = F.linear(hidden_states, self.weight, None)  # å¯¹æ¯ä¸ª token å’Œæ‰€æœ‰ä¸“å®¶æƒé‡è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°æ¯ä¸ª token åˆ†é…ç»™æ¯ä¸ªä¸“å®¶çš„åŸå§‹åˆ†æ•°ï¼ˆlogitsï¼‰ [batch_size*seq_len, n_routed_experts]
        
        # 1. è®¡ç®—åˆ†æ•°ï¼Œå¯¹ä¸“å®¶ç»´åº¦åš softmaxï¼Œå¾—åˆ°æ¯ä¸ª token å¯¹å„ä¸ªä¸“å®¶çš„æ¦‚ç‡
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1)  # å¯¹ logits åº”ç”¨ softmax å‡½æ•°ï¼Œå°†åˆ†æ•°è½¬æ¢ä¸ºä»‹äº 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»£è¡¨æ¯ä¸ª token åˆ†é…ç»™æ¯ä¸ªä¸“å®¶çš„æ¦‚ç‡
        else:
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        # 2. é€‰ top-k ä¸“å®¶
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)  # topk_idx æ˜¯è¢«é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•ï¼Œtopk_weight æ˜¯å¯¹åº”çš„å€¼ï¼ˆprobï¼‰ã€‚[batch_size*seq_len, top_k]

        if self.top_k > 1 and self.norm_topk_prob: # æ˜¯å¦å¯¹ top-k æƒé‡å½’ä¸€åŒ–
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20  
            topk_weight = topk_weight / denominator

        # 3. ä»…è®­ç»ƒé˜¶æ®µï¼Œè®¡ç®—è¾…åŠ©æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶å‡è¡¡ä½¿ç”¨
        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            
            # 3.1 æŒ‰åºåˆ—ç»´åº¦å‡è¡¡
            if self.seq_aux:
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
                
            # 3.2 å…¨å±€è¾…åŠ©æŸå¤±ï¼ˆéåºåˆ—ï¼‰
            else:
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)
                Pi = scores_for_aux.mean(0)
                fi = ce * self.n_routed_experts
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            aux_loss = 0
        return topk_idx, topk_weight, aux_loss # topk_idxï¼šæ¯ä¸ª token çš„ top-k ä¸“å®¶ç´¢å¼•ã€‚topk_weightï¼šå¯¹åº”çš„æƒé‡ã€‚aux_lossï¼šè¾…åŠ©è´Ÿè½½æŸå¤±ï¼Œå¯åŠ å…¥æ€»æŸå¤±å‡½æ•°ä¸­ã€‚


class MOEFeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.gate = MoEGate(config)
        
        # åˆ›å»ºäº†ä¸€ä¸ªç”± n_routed_experts ä¸ª FeedForward ç½‘ç»œç»„æˆçš„åˆ—è¡¨ï¼Œå¹¶æŠŠå®ƒä»¬æ³¨å†Œæˆ nn.Module å­æ¨¡å—ï¼ˆåˆ›å»ºè·¯ç”±ä¸“å®¶åˆ—è¡¨ï¼Œæ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼‰
        self.experts = nn.ModuleList([
            FeedForward(config) for _ in range(config.n_routed_experts) # nn.ModuleList([FeedForward(config), FeedForward(config), FeedForward(config), ...å…±æœ‰ n_routed_experts ä¸ª])
        ])
        
        # å¯é€‰ï¼šå…±äº«ä¸“å®¶ç½‘ç»œï¼ˆä½œç”¨äºæ¯ä¸ª tokenï¼‰
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config) for _ in range(config.n_shared_experts)
            ])

    # token -> gate -> é€‰ä¸“å®¶ -> expert å¤„ç† -> åŠ æƒæ±‚å’Œ -> token è¾“å‡º
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ï¼Œå®ç°æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰çš„è®¡ç®—é€»è¾‘
        å‚æ•°:
        x (Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, hidden_dim]
        è¿”å›:
        y (Tensor): è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        å±æ€§:
        - é€šè¿‡é—¨æ§æœºåˆ¶åŠ¨æ€é€‰æ‹©ä¸“å®¶å­ç½‘ç»œ
        - æ”¯æŒè®­ç»ƒå’Œæ¨ç†ä¸¤ç§è®¡ç®—æ¨¡å¼
        - åŒ…å«è¾…åŠ©æŸå¤±è®¡ç®—å’Œå…±äº«ä¸“å®¶åˆ†æ”¯
        """
        identity = x # ç”¨äº residual åŠ ä¸Šå…±äº«ä¸“å®¶è¾“å‡ºï¼Œ[bsz, seq_len, hidden_size]
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        
        # ===== 1. é—¨æ§é˜¶æ®µï¼šé€‰æ‹© top-k ä¸ªä¸“å®¶ ä½œä¸ºæ¯ä¸ª token çš„è·¯ç”± =====
        # topk_idx: [bsz*seq_len, top_k]ï¼Œæ¯ä¸ª token (å…±bsz*seq_lenä¸ªtoken) é€‰æ‹©çš„ä¸“å®¶ç´¢å¼•
        # topk_weight: [bsz*seq_len, top_k]ï¼Œæ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶å¯¹åº”çš„æƒé‡ï¼ˆé€šå¸¸æ˜¯ softmax åå¾—åˆ°çš„æ¦‚ç‡ï¼‰
        # aux_loss: è´Ÿè½½å‡è¡¡æŸå¤±,è®©æ¯ä¸ªä¸“å®¶å¤§è‡´è¢«åˆ†åˆ° ç›¸åŒæ•°é‡çš„ tokenï¼Œä¸”åˆ†é…çš„æƒé‡å’Œå‡è¡¡(é¼“åŠ±æ¨¡å‹å‡åŒ€ä½¿ç”¨æ‰€æœ‰ä¸“å®¶ï¼Œé˜²æ­¢éƒ¨åˆ†ä¸“å®¶è¿‡è½½)
        topk_idx, topk_weight, aux_loss = self.gate(x)
        
        # ===== 2. Flatten token ç»´åº¦ï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç† token =====
        x = x.view(-1, x.shape[-1])  # å°†è¾“å…¥ x å±•å¼€æˆäºŒç»´å¼ é‡ [bsz * seq_len, hidden_size]
        flat_topk_idx = topk_idx.view(-1)  # å°†ä¸“å®¶ç´¢å¼•å±•å¹³æˆä¸€ç»´ [bsz * seq_len * topk]
        
        # ===== 3.1. ä¸“å®¶å·¥ä½œï¼Œè®­ç»ƒé˜¶æ®µ =====
        if self.training:
            # æ¯ä¸ª token è¢«å¤åˆ¶ top_k æ¬¡ï¼Œé€å…¥ä¸åŒä¸“å®¶
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0) # æ˜¾å¼å¤åˆ¶æ¯ä¸ª token K æ¬¡ï¼šå½¢çŠ¶ [bsz * seq_len * topk, hidden]
            y = torch.empty_like(x, dtype=torch.float16) # ç”¨äºæ”¶é›†æ¯ä¸ªä¸“å®¶å¤„ç†åçš„ç»“æœï¼ˆåŠç²¾åº¦ï¼Œå‡å°‘æ˜¾å­˜ï¼›ç¡®ä¿ä¸åç»­ expert è¾“å‡º dtype ä¸€è‡´ï¼‰ï¼Œ
            
            # éå†æ¯ä¸ªä¸“å®¶ï¼Œè®©å…¶å¤„ç†åˆ†é…ç»™å®ƒçš„ token
            for i, expert in enumerate(self.experts):
                # æ‰¾å‡ºå¹¶è¾“å…¥æ‰€æœ‰åˆ†é…ç»™ç¬¬ i ä¸ªä¸“å®¶çš„ token
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)  # ç¡®ä¿ç±»å‹ä¸€è‡´ [bsz * seq_len * topk, hidden]
                
            # æ¯ä¸ª token åœ¨å¤šä¸ªä¸“å®¶çš„è¾“å‡ºåŠ æƒæ±‚å’Œï¼Œå¾—åˆ° token çš„æœ€ç»ˆå‰é¦ˆè¾“å‡º
            y = (
                y.view(*topk_weight.shape, -1) # æ¯ä¸ª token çš„ K ä¸ªä¸“å®¶è¾“å‡ºèšåˆåœ¨ä¸€èµ·ï¼Œæ–¹ä¾¿æŒ‰æƒé‡åŠ æƒ [bsz*seq_len, top_k, hidden_size]
                * topk_weight.unsqueeze(-1) # å¹¿æ’­åˆ° hidden ç»´åº¦ï¼Œä¿è¯æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºå‘é‡éƒ½ä¹˜ä¸Šå¯¹åº”æƒé‡ [bsz*seq_len, top_k, 1]
                ).sum(dim=1) # æ²¿ K ç»´åº¦æ±‚å’Œ [bsz*seq_len, top_k, 1] -> [bsz*seq_len, hidden_size]
            y = y.view(*orig_shape) # è¿˜åŸä¸º [bsz, seq_len, hidden_size]
            
        # ===== 3.2. ä¸“å®¶å·¥ä½œï¼Œæ¨ç†é˜¶æ®µ =====
        else:
            y = self.moe_infer(x,  # [bsz * seq_len, hidden_size]
                               flat_topk_idx,  # [bsz * seq_len * topk]
                               topk_weight.view(-1, 1) # [bsz * seq_len, top_k] --> [bsz * seq_len * top_k, 1]
                               ).view(*orig_shape)
            # æœ€åyï¼š[bsz, seq_len, hidden_size]
            
        # ===== 4. å¯é€‰ï¼šæ·»åŠ å…±äº«ä¸“å®¶çš„è¾“å‡º =====
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        self.aux_loss = aux_loss
        return y

    @torch.no_grad() # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ˆæ¨ç†è·¯å¾„ä¸ä¼šæ›´æ–°å‚æ•°ï¼‰
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """
        æ¨ç†é˜¶æ®µçš„ MoE å‰å‘ä¼ æ’­ã€‚æŒ‰ç…§ä¸“å®¶ç¼–å·å°† token åˆ†ç»„ï¼Œåˆ†åˆ«é€å…¥å¯¹åº”ä¸“å®¶ä¸­è®¡ç®—ååˆå¹¶ã€‚
        
        å‚æ•°ï¼š
        - x: [bsz * seq_len, hidden_size]             æ‰€æœ‰ token çš„è¡¨ç¤ºï¼ˆæ²¡æœ‰å¤åˆ¶ï¼‰
        - flat_expert_indices: [bsz * seq_len * top_k]  æ¯ä¸ª token è¢«è·¯ç”±åˆ°çš„ä¸“å®¶ç¼–å·
        - flat_expert_weights: [bsz * seq_len * top_k , 1] æ¯ä¸ªä¸“å®¶å¯¹åº”çš„é—¨æ§æƒé‡
        """
        
        # åˆå§‹åŒ–è¾“å‡ºç¼“å­˜ï¼Œä¸ x åŒ shape å’Œ dtype 
        expert_cache = torch.zeros_like(x) # [bsz * seq_len, hidden_size]
        # 1. æ ¹æ®ä¸“å®¶ç¼–å·å¯¹æ‰€æœ‰ token æ’åºï¼ˆä¸ºäº†æŠŠåˆ†é…åˆ°ç›¸åŒä¸“å®¶çš„ token æ”¾åˆ°ä¸€èµ·ï¼‰
        idxs = flat_expert_indices.argsort()
        # 2. ç»Ÿè®¡æ¯ä¸ªä¸“å®¶åˆ†é…åˆ°çš„ token æ•°é‡å¹¶ç´¯åŠ ï¼Œæ–¹ä¾¿åˆ‡åˆ†
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0) # ä¾‹FEI = [0, 1, 0, 2, 1, 3]-> .bincount() = [2, 2, 1, 1]-> .cumsum(0) = [2, 4, 5, 6]ã€‚åˆ™ä¸“å®¶0çš„ token åœ¨æ’åºåçš„ flat ç´¢å¼•èŒƒå›´[0:2],ä¸“å®¶1[2:4],ä¸“å®¶2[4:5],ä¸“å®¶3[5:6]
        # 3. è®¡ç®—æŒ‰ç…§ä¸“å®¶åˆ†ç»„æ’åºåçš„ token å±äºå“ªäº›åŸå§‹ tokenï¼ˆå› ä¸ºæ¯ä¸ª token ä¼šè¢«å¤åˆ¶ top_k æ¬¡ï¼‰
        token_idxs = idxs // self.config.num_experts_per_tok
        
        # 4. éå†æ¯ä¸ªä¸“å®¶ï¼Œå°†åˆ†é…åˆ°è¯¥ä¸“å®¶çš„ token é€å…¥å¯¹åº” FFN è®¡ç®—
        # å½“tokens_per_expert = [6, 15, 20, 26]ï¼Œtokens_per_expert.shape[0]å³ä¸ºä¸“å®¶æ•°é‡ï¼ˆæ­¤æ—¶ä¸º4ï¼‰
        # ä¸”token_idxs = [3, 7, 19, 21, 24, 25,  4,  5,  6, 10, 11, 12...] æ—¶
        # æ„å‘³token_idxs[:6] -> [3, 7, 19, 21, 24, 25]è¿™6ä¸ªä½ç½®å±äºä¸“å®¶0å¤„ç†çš„tokenï¼ˆæ¯ä¸ªtokenæœ‰å¯èƒ½è¢«å¤šä¸ªä¸“å®¶å¤„ç†ï¼Œè¿™å–å†³äºnum_experts_per_tokï¼‰
        # æ¥ä¸‹æ¥9ä¸ªä½ç½®token_idxs[6:15] -> [4,  5,  6, 10, 11, 12...]å±äºä¸“å®¶1å¤„ç†çš„token...ä¾æ­¤ç±»æ¨
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            if start_idx == end_idx:
                continue
            expert = self.experts[i] # å–å‡ºç¬¬ i ä¸ªä¸“å®¶ FFN
            # 5. è·å–åˆ†é…ç»™ç¬¬ i ä¸ªä¸“å®¶çš„ token åŸå§‹ä½ç½®ç´¢å¼•
            exp_token_idx = token_idxs[start_idx:end_idx] # è¯¥ä¸“å®¶è´Ÿè´£çš„ä¸€ç»„ token ç´¢å¼•
            # 6. å–å‡ºå¯¹åº” token è¡¨ç¤º
            expert_tokens = x[exp_token_idx] # å–å‡ºè¯¥ä¸“å®¶å¯¹åº” token çš„è¾“å…¥åˆ‡ç‰‡ï¼šå½¢çŠ¶ [num_token_i, hidden_size]
            # 7. æ‰§è¡Œå½“å‰ä¸“å®¶å¯¹åº” FFN çš„å‰å‘ä¼ æ’­ï¼Œå¹¶è½¬æˆç¼“å­˜çš„ dtype
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            # 8. ç”¨ gate æƒé‡ç¼©æ”¾è¾“å‡º
            # [num_token_i, hidden_size] * [num_token_i, 1] --> [num_token_i, hidden_size]
            expert_out.mul_(
                flat_expert_weights[idxs[start_idx:end_idx]] # å–å‡ºå½“å‰ä¸“å®¶å¤„ç†çš„ tokenå¯¹åº”çš„æƒé‡
                ) # MoE çš„è¾“å‡ºå…¬å¼æ˜¯åŠ æƒæ±‚å’Œã€‚åœ¨æ¨ç†æ—¶ï¼Œæ¯ä¸ªä¸“å®¶åªå¤„ç†è‡ªå·±åˆ†é…åˆ°çš„ tokenï¼Œæ‰€ä»¥å…ˆæŠŠ expert_out ä¹˜ä»¥å¯¹åº”æƒé‡ï¼Œåé¢ scatter_add_ ç´¯åŠ åˆ°æœ€ç»ˆè¾“å‡ºå°±ä¸éœ€è¦å†é‡å¤ä¹˜æƒé‡äº†
            
            # 9. ç´¯åŠ åˆ°è¾“å‡ºç¼“å­˜ä¸­ï¼Œæ”¯æŒä¸€ä¸ª token è¢«å¤šä¸ªä¸“å®¶å¤„ç†åç»“æœå åŠ (æ¯ä¸ª token ä¼šè¢«é€åˆ°è‹¥å¹²ä¸ªä¸“å®¶, ä¸“å®¶è¾“å‡ºç»“æœåï¼Œéœ€è¦â€œè·¯ç”±â€å› token çš„åŸå§‹ä½ç½®ï¼Œå¹¶ä¸”åŠ æƒ)
            expert_cache.scatter_add_(
                0, # dim
                exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), # å­˜æ”¾â€œè¿™ä¸ªä¸“å®¶è¾“å‡ºå¯¹åº”å›å“ªä¸ª tokenâ€
                expert_out # ä¸“å®¶çš„è¾“å‡ºç»“æœï¼ˆå·²ä¹˜ä»¥æƒé‡ï¼‰
                )

        return expert_cache


# å¯¹åº”MiniMindæ¶æ„å›¾ä¸­çš„Transformer Layer k
class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        # åŸºç¡€æ¨¡å—
        self.num_attention_heads = config.num_attention_heads  # 8
        self.hidden_size = config.hidden_size  # 512
        self.head_dim = config.hidden_size // config.num_attention_heads  # 64
        
        # è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå†…éƒ¨å®ç°äº† RoPE ç›¸å¯¹ä½ç½®ç¼–ç 
        self.self_attn = Attention(config)

        # å½“å‰ Block çš„å±‚ç¼–å·ï¼ˆå¯ç”¨äºå±‚å†…æƒé‡å…±äº«ã€åˆ†å±‚æ§åˆ¶ç­‰ï¼‰
        self.layer_id = layer_id
        
        # Attention å‰çš„ RMSNorm
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # Attention å FFN å‰ RMSNorm
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # å‰é¦ˆç½‘ç»œæ¨¡å—ï¼Œå¯é…ç½®æ˜¯å¦ä½¿ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, 
                hidden_states,  # è¾“å…¥çš„éšè—çŠ¶æ€ [batch_size, seq_len, hidden_dim]
                position_embeddings,  # RoPE ä½ç½®ç¼–ç  [seq_len, head_dim]
                past_key_value=None,  # KV ç¼“å­˜ (key, value)ï¼Œç”¨äºåŠ é€Ÿæ¨ç†
                use_cache=False,  # æ˜¯å¦ç¼“å­˜å½“å‰å±‚çš„ KV
                attention_mask=None  # attention æ©ç 
                ):
        # ---------------------- Self-Attention å±‚ ----------------------
        residual = hidden_states  # ä¿å­˜æ®‹å·®è¿æ¥
        
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states),  # RMSNorm
            position_embeddings,  # Rotary PE ä¼ å…¥ Attention
            past_key_value,  # ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„keyå’Œvalueç¼“å­˜ï¼Œç”¨äºåŠ é€Ÿè§£ç 
            use_cache,  # æ˜¯å¦ç¼“å­˜å½“å‰å±‚ KVï¼ˆä¸€èˆ¬åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨ï¼‰
            attention_mask  # æ³¨æ„åŠ›æ©ç ï¼ˆpadding tokenä¸è®¡ç®—æ³¨æ„åŠ›çŸ©é˜µï¼‰
        )
        
        # æ®‹å·®è¿æ¥ï¼šåŸå§‹è¾“å…¥ + attention è¾“å‡º
        hidden_states += residual
        
        # ---------------------- MLP å±‚ ----------------------
        # MLP å‰å†åšä¸€æ¬¡ RMSNorm
        hidden_states = hidden_states 
        + self.mlp(self.post_attention_layernorm(hidden_states))  # MLP å‰å†åšä¸€æ¬¡ RMSNorm
        
        # è¿”å›æ–°çš„ hidden_states å’Œ å½“å‰å±‚çš„ KV ç¼“å­˜
        return hidden_states, present_key_value

# æ•´ä¸ª Transformer ä¸»å¹²ç½‘ç»œï¼Œç”±å¤šå±‚ MiniMindBlock å †å è€Œæˆ
class MiniMindModel(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)  # ç”¨äºå°† token id æ˜ å°„ä¸ºå¯è®­ç»ƒçš„å‘é‡, åˆå§‹åŒ–é»˜è®¤å‡åŒ€åˆ†å¸ƒ, [vocab_size, hidden_size]ã€‚
        self.dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])  # æ„å»º num_hidden_layers ä¸ª Transformer Block å±‚
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)  # Transformer åçš„ LayerNorm å±‚

        # é¢„è®¡ç®— RoPE æ‰€éœ€çš„ä½ç½®é¢‘ç‡å‘é‡ (cos/sin)
        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,
                                                    end=config.max_position_embeddings, theta=config.rope_theta)
        # æ³¨å†Œä¸º bufferï¼ˆæ¨¡å‹ä¸­æŒä¹…å­˜å‚¨ä½†ä¸å‚ä¸ä¼˜åŒ–ï¼‰
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)  # æ³¨å†Œä¸º bufferï¼ˆæ¨¡å‹ä¸­æŒä¹…å­˜å‚¨ä½†ä¸å‚ä¸ä¼˜åŒ–ï¼‰
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)  # æ³¨å†Œä¸º bufferï¼ˆæ¨¡å‹ä¸­æŒä¹…å­˜å‚¨ä½†ä¸å‚ä¸ä¼˜åŒ–ï¼‰

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,  # è¾“å…¥åºåˆ—ï¼Œ[batch_size, seq_len]
                attention_mask: Optional[torch.Tensor] = None,  # padding mask [batch_size, seq_len]
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,  # ç¼“å­˜ è¿‡å»çš„ KV [(k_cache, v_cache),...]ï¼Œé•¿åº¦ç­‰äºå±‚æ•°=num_hidden_layers
                use_cache: bool = False,
                **kwargs):
        batch_size, seq_length = input_ids.shape
        # past_key_values = past_key_values or [None] * len(self.layers)
        if past_key_values is None or past_key_values == []:  # å¦‚æœæ²¡ä¼ å…¥ç¼“å­˜ï¼Œåˆå§‹åŒ–ä¸ºç©ºï¼ˆæ¨ç†æ—¶æ‰ä½¿ç”¨KVç¼“å­˜ï¼‰
            past_key_values = [None] * len(self.layers)  # [None, None, ..., None]ï¼Œé•¿åº¦ç­‰äºå±‚æ•°=num_hidden_layers
        
        # ä¸ºæ¯ä¸ªæ–°ç”Ÿæˆçš„ token è®¡ç®—æ­£ç¡®çš„ä½ç½®ç¼–ç èµ·å§‹ä½ç½®
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0  # past_seq_len

        hidden_states = self.dropout(
            self.embed_tokens(input_ids)  # æ³¨æ„è¿™ä¸æ˜¯çŸ©é˜µè¿ç®—ï¼Œè€Œæ˜¯æŸ¥è¡¨æ“ä½œï¼šä¾‹å¦‚ input_ids=[[1,3]]ï¼Œåˆ™ embed_tokens(input_ids)=[embed_tokensç¬¬1è¡Œ, embed_tokensç¬¬3è¡Œ]
            )  

        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        presents = []  # å­˜å‚¨ KV cacheï¼šæ¯å±‚ä¸€ä¸ª (key, value)
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        hidden_states = self.norm(hidden_states)

        # å¦‚æœä½¿ç”¨äº† MOEï¼ˆç¨€ç–ä¸“å®¶ï¼‰ï¼Œåˆ™åˆå¹¶è¾…åŠ©æŸå¤±
        aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if isinstance(layer.mlp, MOEFeedForward)
        )

        return hidden_states, presents, aux_loss


# é¢å‘å› æœè¯­è¨€å»ºæ¨¡çš„æ¥å£å°è£…ï¼ˆHugging Face é£æ ¼ï¼‰
class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):  # ç»§æ‰¿äº† GenerationMixinï¼Œå…¶é‡Œé¢å®ç°äº† è‡ªå›å½’ç”Ÿæˆé€»è¾‘ï¼ˆé¢„æµ‹ â†’ æ‹¼æ¥ â†’ å†é¢„æµ‹ï¼‰
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        
        # æ¨¡å‹ä¸»å¹²ï¼šMiniMindModelï¼Œè¾“å‡º hidden_states
        self.model = MiniMindModel(self.config)

        # è¾“å‡ºå±‚ï¼šå°† hidden_size æ˜ å°„ä¸º vocab_sizeï¼ˆå³æ¯ä¸ª token çš„ logitsï¼‰
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)  # [vocab_size, hidden_size]

        # æƒé‡ç»‘å®šï¼šembedding æƒé‡ä¸ lm_head æƒé‡å…±äº«
        self.model.embed_tokens.weight = self.lm_head.weight  # å…±äº«æƒé‡ï¼Œå‡å°‘å‚æ•°é‡ [vocab_size, hidden_size]

        # è¾“å‡ºå®¹å™¨ï¼ˆHugging Face Transformers å®šä¹‰çš„æ ‡å‡†è¾“å‡ºå¯¹è±¡ï¼‰
        self.OUT = CausalLMOutputWithPast()

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,  # [batch_size, seq_len]
                attention_mask: Optional[torch.Tensor] = None,  # [batch_size, seq_len]
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,  # æ§åˆ¶ logits ä¿ç•™å“ªäº› tokenï¼Œä¸€èˆ¬è®­ç»ƒæ—¶è®¾ç½®ä¸º0ï¼Œæ¨ç†æ—¶è®¾ç½®ä¸º1
                **args):
        
        # è°ƒç”¨ä¸»å¹²æ¨¡å‹ï¼Œè¾“å‡º hidden_statesã€presentsï¼ˆKVç¼“å­˜ï¼‰ã€aux_loss
        h, present_kvs, aux_loss = self.model(
            input_ids=input_ids,                                            # è¾“å…¥ token åºåˆ—
            attention_mask=attention_mask,                                  # ç”¨äº mask padding çš„ attention mask
            past_key_values=past_key_values,                                # ç”¨äºå¢é‡æ¨ç†çš„ KV ç¼“å­˜
            use_cache=use_cache,                                            # æ˜¯å¦è¿”å› KV cache
            **args
        )
        
        # æ ¹æ® logits_to_keep å‚æ•°å†³å®šä¿ç•™è¾“å‡ºçš„å“ªäº›ä½ç½®
        # slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        if isinstance(logits_to_keep, int):  # å½“ logits_to_keep æ˜¯æ•´æ•°æ—¶
            # å–åºåˆ—æœ€å logits_to_keep ä¸ª token
            slice_indices = slice(-logits_to_keep, None)  # -logits_to_keep è¡¨ç¤º ä»åºåˆ—å€’æ•°ç¬¬ logits_to_keep ä¸ª token å¼€å§‹; None è¡¨ç¤ºä¸€ç›´åˆ°åºåˆ—æœ«å°¾
        else:  # å¦‚æœ logits_to_keep ä¸æ˜¯æ•´æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ª tensor æˆ–ç´¢å¼•åˆ—è¡¨ï¼Œå°±ç›´æ¥ç”¨å®ƒåšåˆ‡ç‰‡
            slice_indices = logits_to_keep
        
        # ä» h ä¸­ä¿ç•™æœ€å logits_to_keep ä¸ªtokenï¼Œé€å…¥ lm_head åšåˆ†ç±»
        logits = self.lm_head(h[:, slice_indices, :]) # è®­ç»ƒæ—¶ï¼Œslice_indices æ˜¯ 0ï¼Œlogits ç›¸å½“äº self.lm_head(h[:, 0:, :])ï¼Œå³æ•´ä¸ª h;æ¨ç†æ˜¯1ï¼Œlogits ç›¸å½“äº self.lm_head(h[:, -1:, :])ï¼Œå³æœ€åä¸€ä¸ª token çš„ hidden_state
        # logits.shape: [batch_size, logits_to_keep, vocab_size]        self.OUT.__setitem__('last_hidden_state', h)
        
        # æ„å»ºç»“æ„åŒ–è¾“å‡ºå­—å…¸
        self.OUT.__setitem__('last_hidden_state', h)          # [batch_size, seq_len, hidden_size]
        self.OUT.__setitem__('logits', logits)                # [batch_size, logits_to_keep, vocab_size]
        self.OUT.__setitem__('aux_loss', aux_loss)            # scalar or tensor
        self.OUT.__setitem__('present_key_values', present_kvs)     # list of tuples: (key, value)
        return self.OUT
